{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clementbowe14/businessanalytics/blob/main/Lab_C_Fitting_the_Best_Model_(Student).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Statement\n",
        "The data scientists at BigMart have collected 2013 sales data for 1559 products across 10 stores in different cities. Also, certain attributes of each product and store have been defined. The aim is to build a predictive model and find out the **sales** of each product at a particular store. Using this model, BigMart will try to understand the properties of products and stores which play a key role in increasing sales. Please note that the data may have missing values as some stores might not report all the data due to technical glitches. Hence, it will be required to treat them accordingly.\n",
        "\n",
        "## Variables\n",
        "\n",
        "Item_Identifier : Unique product ID\n",
        "\n",
        "Item_Weight : Weight of product\n",
        "\n",
        "Item_Fat_Content : Whether the product is low fat or not\n",
        "\n",
        "Item_Visibility : The % of total display area of all products in a store allocated to the particular product\n",
        "\n",
        "Item_Type : The category to which the product belongs\n",
        "\n",
        "Item_MRP : Maximum Retail Price (list price) of the product\n",
        "\n",
        "Outlet_Identifier : Unique store ID\n",
        "\n",
        "Outlet_Establishment_Year : The year in which store was established\n",
        "\n",
        "Outlet_Size : The size of the store in terms of ground area covered\n",
        "\n",
        "Outlet_Location_Type : The type of city in which the store is located\n",
        "\n",
        "Outlet_Type : Whether the outlet is just a grocery store or some sort of supermarket\n",
        "\n",
        "Item_Outlet_Sales : Sales of the product in the particulat store. This is the outcome variable to be predicted."
      ],
      "metadata": {
        "id": "Mom7DAuTbF0t"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G_3j-aATn4o",
        "outputId": "bfb1f2f8-0096-4f72-9140-e7be9a538e22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install dmba"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dmba\n",
            "  Downloading dmba-0.1.0-py3-none-any.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dmba\n",
            "Successfully installed dmba-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WVVcDzJ5UG4"
      },
      "source": [
        "!pip install autoviz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ux12SepD5ZDa"
      },
      "source": [
        "pip install missingpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gEXuI3xTmsX"
      },
      "source": [
        "%matplotlib inline\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, BayesianRidge, ElasticNetCV\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import sklearn.metrics\n",
        "\n",
        "from dmba import regressionSummary, gainsChart, liftChart\n",
        "from dmba import adjusted_r2_score, AIC_score, BIC_score\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnrjdomdTmsd"
      },
      "source": [
        "data_df = pd.read_csv('https://raw.githubusercontent.com/dguggen/BADM211_datasets/main/Big_Mart_Sales.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0w-XGfJaTmse"
      },
      "source": [
        "# Display the first 10 rows of the dataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpMOIRTVTmsg"
      },
      "source": [
        "# get rid of nonessential variables - but why are we removing these variables?\n",
        "\n",
        "data_df = data_df.drop(???, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67NzZk8kTmsh"
      },
      "source": [
        "# Check for missing values using the bar chart format\n",
        "%matplotlib inline\n",
        "import missingno as msno\n",
        "\n",
        "msno.???(???, color='darkorange')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGUWBm1yTmsl"
      },
      "source": [
        "# isolate predictors separate from the response variable, and we're using all predictors\n",
        "\n",
        "X = data_df.???(columns=[???])\n",
        "\n",
        "y = data_df[???]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xxuc2D7dTmsm"
      },
      "source": [
        "# dummy code the predictors; THIS STEP OCCURS BEFORE TRAIN/TEST SPLIT\n",
        "\n",
        "X = pd.get_dummies(???, drop_first = ???)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZ5bIhrvTmsn"
      },
      "source": [
        "# Check out how many samples and predictors after one-hot encoding\n",
        "\n",
        "???"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SUTB6EATmso"
      },
      "source": [
        "# let's take a look at all of the predictors after dummy coding; what measures do these employ?\n",
        "\n",
        "pd.DataFrame(???)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxuEnHPNTmsp"
      },
      "source": [
        "## CV for MLR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvYpPPaZTmsq"
      },
      "source": [
        "# MISSING VALUE IMPUTATION HERE \n",
        "\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "imputer = ???(n_neighbors=5)\n",
        "\n",
        "X_imp = ???.fit_transform(???)\n",
        "\n",
        "X_imp = pd.DataFrame(X_imp, columns = X.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# re-check missing values after imputation\n",
        "\n",
        "msno.bar(???, color='darkorange')"
      ],
      "metadata": {
        "id": "H8fmfgD7gxyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up testing and training sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_imp, y, test_size=0.25, random_state=27)"
      ],
      "metadata": {
        "id": "GugDWnF51hJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We’ll use the following function to get cross validation scores for our models:"
      ],
      "metadata": {
        "id": "aW_GpCAlwYW4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ptqF37eTmsr"
      },
      "source": [
        "# creat a function to get cross validation scores\n",
        "def get_cv_scores(model):\n",
        "    scores = cross_val_score(model,\n",
        "                             X_train,\n",
        "                             y_train,\n",
        "                             cv=???, # 5-fold CV\n",
        "    \n",
        "    print('CV Mean: ', np.mean(scores))\n",
        "    print('STD: ', np.std(scores))\n",
        "    print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To generate a linear regression, we use Scikit-Learn’s LinearRegression class:"
      ],
      "metadata": {
        "id": "1NV9vBsf0PW_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgltGvGrTmsr"
      },
      "source": [
        "\n",
        "# Train model\n",
        "lr = LinearRegression().fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# get cross val scores\n",
        "get_cv_scores(lr)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discuss\n",
        "- **What does the standard deviation value indicate?**\n",
        "- **Are we overfitting the data?**\n",
        "\n"
      ],
      "metadata": {
        "id": "q9_Kge9yloZo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overfitting occurs when the model makes much better predictions on known data than on unknown data. The model begins to memorize the training data and is unable to generalize to unseen test data.\n",
        "\n",
        "One option to combat overfitting is to simplify the model. We’ll attempt to simplify our linear regression model by introducing regularization.\n",
        "Regularization can be defined as explicitly restricting a model to prevent overfitting.\n",
        "\n",
        "As linear regression has no parameters, there is no way to control the complexity of the model. We’ll explore some variations that add **regularization** (next week).\n",
        "\n"
      ],
      "metadata": {
        "id": "uqBqSW8MlsoX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQLCXv4qTms1"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}